{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona PROPN nsubj\n",
      "will AUX aux\n",
      "go VERB ROOT\n",
      "very ADV advmod\n",
      "soon ADV advmod\n",
      ". PUNCT punct\n",
      "Do AUX aux\n",
      "not PART neg\n",
      "get VERB ROOT\n",
      "panic NOUN dobj\n",
      ", PUNCT punct\n",
      "maintain VERB conj\n",
      "social ADJ amod\n",
      "distancing NOUN dobj\n",
      "and CCONJ cc\n",
      "follow VERB conj\n",
      "the DET det\n",
      "instructions NOUN dobj\n",
      ". PUNCT punct\n",
      "Cases NOUN nsubj\n",
      "in ADP prep\n",
      "U.S. PROPN pobj\n",
      "have AUX aux\n",
      "reduced VERB ROOT\n",
      "in ADP prep\n",
      "last ADJ amod\n",
      "48 NUM nummod\n",
      "hours NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "#Create a Doc object\n",
    "doc = nlp(u'Corona will go very soon. Do not get panic, maintain social distancing and follow the instructions. Cases in U.S. have reduced in last 48 hours')\n",
    "\n",
    "#Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1758ee95540>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1758ee8e130>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1758ebea9a0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1758ebead60>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1758ef0edc0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1758ef1f180>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN advcl\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Apple isn't looking at buying U.K. startup.\")\n",
    "for token in doc3:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is AUX\n",
      "n't PART\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "for token in doc4:     \n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "at buying U.K."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "sliced_text = doc5[4:7]\n",
    "sliced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
    "for sent in doc6.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
